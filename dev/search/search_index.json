{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>utia-gc/scrnaseq</code>","text":"<p>Welcome to the <code>utia-gc/scrnaseq</code> documentation!</p>"},{"location":"#structure-of-the-docs","title":"Structure of the docs","text":"<p>These docs are mainly setup in a question and answer format, typically from the perspective of a user who has decided to run the pipeline and is asking themself a question starting with \"How do I... ?\"</p> <p>In order to properly answer these questions, we will take on each in turn and state why it is a problem that we felt the need to address in our pipeline. From there, we will describe the solution in as plain terms as possible so that the user has a mental model of what the pipeline is actually doing. Where possible, we will also point to a place in the pipeline code where the solution is implemented; this way the user can compare their mental model of the solution with the way the solution is actually written. Finally, we will give direction as to how the user can make use of our solution to ensure their pipeline run works as intended.</p>"},{"location":"#questions","title":"Questions","text":""},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#input-output","title":"Input / Output","text":"<ul> <li> <p>What params do I need to run the pipeline?</p> </li> <li> <p>How do I format the input samplesheet?</p> </li> <li> <p>What outputs will I get from the pipeline?</p> </li> <li> <p>How should I setup my project?</p> </li> </ul>"},{"location":"#pipeline-configuration","title":"Pipeline Configuration","text":"<ul> <li> <p>How do I run a step of the pipeline with the command line arguments that I want to use?</p> </li> <li> <p>How do I make it easier to try out and evaluate new params or arguments?</p> </li> <li> <p>What parameters are available?</p> </li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<ul> <li>How can I contribute to the pipeline code or documentation?</li> </ul>"},{"location":"#problems-with-code-and-docs","title":"Problems with code and docs","text":"<p>Since these docs are written and maintained by the pipeline developers, there will be many great questions from the users that need answers but which we haven't thought of asking. In this case, please open a new issue in the main repo issues page so that we can make sure the pipeline is useful to and usable by everyone. There is no such thing as a dumb question!</p> <p>We also kindly ask that you report any bugs you may come across and make any feature requests in the issues page as well.</p>"},{"location":"contribute/","title":"Contribute","text":""},{"location":"contribute/create_new_pipeline/","title":"Create New Pipeline","text":"<p><code>utia-gc/scrnaseq</code> and pipelines built on <code>utia-gc/ngs</code> are meant to be readily and easily extensible to facilitate quick development of pipelines for processing NGS data. This document provides recommendations for creating a new pipeline based on <code>utia-gc/scrnaseq</code>.</p>"},{"location":"contribute/create_new_pipeline/#fork-the-pipeline-repository","title":"Fork the pipeline repository","text":"<p>The easiest way to create a new pipeline is to make a fork of the existing pipeline on GitHub.</p> <ol> <li>Navigate to the pipeline's GitHub repository</li> <li>Check to see if a pipeline matching your needs already exists -- Click the arrow next to the Fork button in the top right corner of the page to show a list of all the pipelines associated with <code>utia-gc/scrnaseq</code>.</li> <li>If you need to create a new pipeline, click the <code>Create a new fork</code> button below the dropdown list or just the <code>Fork</code> button at the top right of the repo code page.     Note this requires a GitHub account.     If you aren't logged in you will be redirected to the login page where you must either login or create a GitHub account if you don't have one already.</li> <li>Follow the prompts to create your fork with your desired pipeline name and description.     I recommend only copying the <code>main</code> branch, which should be selected by default.</li> </ol>"},{"location":"contribute/create_new_pipeline/#make-a-local-clone-of-your-pipeline-repository","title":"Make a local clone of your pipeline repository","text":"<p>To effectively edit your pipeline, make a local clone of your pipeline's repository on the computer where you will develop your pipeline (your development environment).</p> <ol> <li>Copy your pipeline URL -- Click the green <code>Code</code> button in the top right of your pipeline repo code page and copy the URL. I typically use the HTTPS.</li> <li>Open a terminal session in your development environment and navigate to the directory where you want to create the clone of your pipeline repo.</li> <li> <p>Clone your pipeline repo:</p> Terminal<pre><code>git clone --recurse-submodules &lt;repository-URL&gt;\n</code></pre> <p><code>--recurse-submodules</code></p> <p>All local test data is located in <code>tests/data</code> which is setup as a git submodule. Supplying the <code>--recurse-submodules</code> argument to <code>git clone</code> will clone the correct repository into <code>tests/data</code> so that you can run tests.</p> <p>If you don't clone submodules at this step it's possible to do this later, but it's easier to do it at this project setup unless you have a reason not to.</p> </li> </ol>"},{"location":"contribute/create_new_pipeline/#change-pipeline-name","title":"Change pipeline name","text":"<p>One of the first changes that needs to be made is updating the pipeline name throughout the repo.</p> <p>In most places this isn't strictly necessary as the repository name is primarily found in comments and the docs, but it's still best to update it at this point, otherwise you'll probably forget to update it ever which will create confusion down the line.</p> <p>How exactly you do this depends on your environment and what you find easiest. I typically in the IDE VS Code, and I like the Find in file feature that it has available in the file explorer paine for searching and replacing. <code>grep</code>ping for the old repo name and manually editing files is another fine solution.</p> <p>Don't blindly find and replace</p> <p>I very explicitly recommend not doing a blind, automated search and replace with a tool like <code>sed</code> or in the IDE of your choice. You never know what exactly is being found and replaced in those cases, so you can accidentally break something. For example, if you blindly replace all instances of <code>ngs</code> in your repo with <code>atacseq</code> then an important reference to a <code>settings.config</code> file would instead reference <code>settiatacseq.py</code> and wreak who knows what havoc.</p> <p>Take a few minutes to do things by hand -- it's better to set things up correctly to begin with.</p>"},{"location":"contribute/create_new_pipeline/#setup-virtual-environment","title":"Setup virtual environment","text":"<p>Developing your pipeline will involve a few Python tools, and Python best practices demands the use of virtual environments.</p> <ol> <li> <p>Create the virtual environment. I recommend using the <code>venv</code> module to create a virtual environment in the <code>.venv</code> directory. This module does everything we need and ships with Python and therefore doesn't require any additional installations.</p> Terminal<pre><code>python3 -m venv .venv\n</code></pre> </li> <li> <p>Activate the virtual environment. You should activate your virtual environment every time you develop in this repository. A good IDE like VS Code can handle this for you so that activation is automated.</p> Terminal<pre><code>source .venv/bin/activate\n</code></pre> </li> <li> <p>Install packages into the virtual environment Now that your virtual environment is setup, all that's left to do is install the necessary packages in your environment so that you can use them. This is easy to do with the <code>requirements.txt</code> that is forked and cloned inside your new pipeline repo.</p> Terminal<pre><code>python3 -m pip install -r requirements.txt\n</code></pre> </li> </ol>"},{"location":"contribute/create_new_pipeline/#deploy-documentation","title":"Deploy documentation","text":"<p>Documentation for <code>utia-gc/scrnaseq</code> is deployed on GitHub Pages from a branch that is maintained by GitHub Actions. This means that you set up document deployment once and you shouldn't have to think about manually deploying documentation again.</p> <ol> <li> <p>Enable GitHub actions in your repo</p> <ol> <li>Navigate to your pipeline repo on GitHub</li> <li>Select the <code>Actions</code> tab</li> <li>If there is a message saying Actions have been disabled, click the big green button in the center of the screen to enable actions.</li> </ol> </li> <li> <p>Make <code>gh-pages</code> branch. The easiest way to deploy your docs to GitHub Pages will be to deploy them from a branch, specifically the <code>gh-pages</code> branch (1) . The easiest way to do this is to simply make a new commit and push it to your main branch. This will trigger the GitHub Action for building the dev version of your docs. When the workflow is completed, check your repo on GitHub and verify that a <code>gh-pages</code> branch (2) exists.</p> <ol> <li>There is nothing special about this branch name other than the fact that it is the branch name for the docs that get built by MkDocs when the GitHub Action runs. We strongly recommend using the <code>gh-pages</code> branch name for this reason.</li> <li>This branch will be on your remote repository on GitHub, and I recommend leaving it there. You won't have much of a reason to create a local copy and edit it since this branch will be constantly rebuilt and redeployed as you make new commits on <code>origin/main</code> and create release branches.</li> </ol> </li> <li> <p>Set GitHub Pages to deploy from a branch.</p> <ol> <li>From your repository home page, go to the pages settings: <code>Settings</code> &gt; <code>Pages</code></li> <li>Under \"Build and deployment\" select Source: \"Deploy from a branch\" (should be selected by default)</li> <li>Under Branch select <code>gh-pages</code>, then \"/ (root)\"</li> <li>Click <code>Save</code></li> </ol> </li> <li> <p>Update the docs link in the sidebar and README. It will likely take a few minutes for GitHub Pages to integrate changes from the deployment branch to the actual website. Once your site is live, the <code>Settings</code> &gt; <code>Pages</code> page will have a link to your docs site. Visit that link to make sure everything looks as expected.</p> </li> </ol> <p>Once you have verified the site looks good, don't forget to update the links to the docs in your repo's About section (top right of the Code page). This is also a good time to verify that the link to the full documentation in the README correctly points to your docs pages.</p>"},{"location":"contribute/create_new_pipeline/#conclusion","title":"Conclusion","text":"<p>By the end of this tutorial you should have the following:</p> <ul> <li>A remote GitHub repository for your new pipeline forked off of <code>utia-gc/scrnaseq</code>.</li> <li>A clone of your GitHub repo in your preferred development environment with a virtual environment created and activated.</li> <li>A working documentation website on GitHub Pages.</li> <li>All references throughout the pipeline and docs should have your pipeline name instead of <code>utia-gc/scrnaseq</code>.</li> </ul>"},{"location":"contribute/development/","title":"Development","text":""},{"location":"contribute/development/#style-conventions","title":"Style conventions","text":"<p>It would be handy to be able to differentiate between modules, subworkflow, and workflows at a glance. To make that happen, use the following naming conventions that imply a hierarchy:</p> <ul> <li>Modules: lower_snake_case</li> <li>Example: <code>samtools_sort</code></li> <li>Subworkflow: Pascal_Snake_Case</li> <li>Example: <code>Prepare_Refs</code></li> <li>Workflow: UPPER_SNAKE_CASE</li> <li>Example: <code>PROCESS_READS</code></li> </ul>"},{"location":"contribute/development/#testing","title":"Testing","text":"<p>Use the <code>nf-test</code> framework for testing.</p>"},{"location":"contribute/development/#what-to-test","title":"What to test","text":"<p>Ideally, all processes, subworkflows, workflows, and pipelines should be tested. I (Trevor) am a proponent of test-driven (specifically behavior-driven) development, which means most everything will be tested as it is implemented. In order to simplify testing, I highly recommend a strategy something like the following:</p> <ul> <li>Processes</li> <li>Test processes for a combination of possible inputs to make sure it behaves as expected.</li> <li>Test that processes emit the expected channels.</li> <li>Do not test that processes produce expected files in the proper publish directories. This behavior should be tested at the workflow and pipeline levels.</li> <li>Subworkflows</li> <li>Same recommendations as processes.</li> <li>Workflows</li> <li>Test workflows for a combination of possible inputs and parameters used in workflows.<ul> <li>If subworkflows or processes can be skipped or behavior relies on parameters (e.g. allowed tools) make sure that it works here.</li> </ul> </li> <li>Test that workflows emit the expected channels.</li> <li>Test that expected files are produced in the proper publish directories.</li> <li>This is generally where the most extensive testing should be done - if workflows behave as expected then ideally so will the subworkflows and processes that comprise them as well the pipelines that they comprise.</li> <li>Pipelines</li> <li>Test that the expected number of tasks succeed.</li> <li>Test that the pipeline fails if given conditions for which it should fail.</li> <li>Test tat expected files are produced in the proper publish directories.</li> </ul>"},{"location":"contribute/documentation/","title":"Documentation","text":"<p>Documentation should be included alongside development so that the project is kept in sync.</p> <p>The documentation is built using Material for MkDocs. This makes documentation easy to write, deploy, and manage as all docs are written in markdown files inside the <code>docs</code> directory. Deployment is handled automatically by GitHub Actions and GitHub pages.</p>"},{"location":"contribute/documentation/#development","title":"Development","text":"<p>Writing docs should be simple.</p> <p>In many cases, an existing page can simply be added to. If a new page needs to be created, then simply create a new markdown file and include its path in the <code>nav</code> section of <code>/mkdocs.yml</code>.</p> <p>In order to serve a test site during development, ensure a virtual environment with the necessary plugins is loaded and execute the following command:</p> Terminal<pre><code>mkdocs serve\n</code></pre>"},{"location":"input_output/","title":"Input / Output","text":""},{"location":"input_output/outputs/","title":"Outputs","text":""},{"location":"input_output/outputs/#a-note-on-terminology","title":"A note on terminology","text":"<p>In Nextflow parlance, output files that are written to specified directories by Nextflow are said to be \"published.\" We will stick with this terminology here for the sake of consistency. That is, files produced by tasks within the pipeline and written to subdirectories of the Nextflow working directory are called \"output files\". Output files that are published to a specified folder are called \"published files\".</p> <p>Nextflow <code>publishDir</code> docs</p>"},{"location":"input_output/outputs/#publish-directories","title":"Publish directories","text":"<p>The purpose of this pipeline is two-fold: first, to perform necessary data processing steps on NGS data, and second, to report important quality control metrics for the NGS data and the processing steps performed here.</p> <p>As such, we feel that it makes sense to publish the results to different directories based on which of these two purposes they fulfill. Data files are published to subdirectories within the directory specified in the parameter <code>publishDirData</code>. We consider data files to be those files which would be indespensible for reanalysis or continued analysis, e.g. fastq files of trimmed and concatenated reads, coordinate sorted BAM files, counts matrices of reads within genes, etc. Reports files are published to subdirectories within the directory specific in the parameter <code>publishDirReports</code>. We consider reports files to be those files which are either reports themselves (MultiQC reports, FastQC reports, etc.) or files that are directly used to make QC reports and are of limited utility elsewhere (e.g. MultiQC data, FastQC zip data, logs from alignment or trimming tools, Samtools flagstat, etc.).</p>"},{"location":"input_output/project_setup/","title":"Project setup","text":""},{"location":"input_output/project_setup/#problem","title":"Problem","text":"<p>How should I setup my project?</p> <p>Many bioinformatics pipelines tend to work best when projects are setup in a specific manner, i.e. when certain files are present and when there is a specific directory structure. This is not the case for <code>utia-gc/scrnaseq</code> and pipelines built on <code>utia-gc/ngs</code>. The pipeline does not assume or require any default directory structure or locations for specific files -- these are all supplied at runtime as params or config options. Ironically, we find that not requiring a specific project setup leads many of us with a new issue -- if all the required inputs can go anywhere, then what's a good place to put them?</p> <p>Even without this analysis paralysis, setting up a new project involves a lot of repetitive work that is susceptible to all sorts of human error and could easily be abstracted away and automated.</p>"},{"location":"input_output/project_setup/#solution","title":"Solution","text":"<p>In order to alleviate analysis paralysis and handle repetitive work, we provide two optional but highly recommended tools.</p> <p>First is cookiecutter, which sets up a new project skeleton from a template. This will initialize a new project directory with our favored directory structure and naming scheme. The project directory that is created contains a convenient README file and scripts for submitting both Nextflow setup and main scripts as jobs to the ISAAC SLURM scheduler. In addition, it has template Nextflow config files and params for running the project setup script (described below). Even if the user does not intend to use the project directory as it is setup in the template, we find that this template is informative for seeing what files the pipeline expects and how to configure runs the of the main pipeline.</p> <p>Second is a convenient Nextflow script (<code>setup.nf</code>) for copying data files into the project directory and setting up the samplesheet that provides pipeline inputs.</p> <p>Both of these tools are packaged with the pipeline code in order to limit external dependencies and keep the tools immediately useable and in-line with their respective pipeline revisions.</p>"},{"location":"input_output/project_setup/#usage","title":"Usage","text":"<p>As stated above, setting up your projects in this way is completely optional, and either of these steps can be run independently of the other. However, we highly recommended using both tools in the two simple steps outline here.</p> <ol> <li> <p>Setup the project directory from the pipeline repo with Cookiecutter:</p> Terminal<pre><code>cookiecutter gh:utia-gc/scrnaseq --directory cookiecutter --checkout main\n</code></pre> </li> <li> <p>Run the project setup Nextflow script:</p> Terminal<pre><code>nextflow run utia-gc/scrnaseq \\\n    -main-script setup.nf \\\n    -revision main \\\n    -params-file src/nextflow/setup_params.yaml\n</code></pre> </li> </ol>"},{"location":"input_output/project_setup/#cookiecutter-project-template","title":"Cookiecutter project template","text":"<p>Using the Cookiecutter project template is very straightforward. The only requirement is an existing Cookiecutter installation on your path. We recommend following the installation instructions using pip inside a virtual environment as dictated by Python best practices.</p> <ol> <li> <p>Install Cookiecutter</p> </li> <li> <p>Setup the project directory from the pipeline repo with Cookiecutter:</p> Terminal<pre><code>cookiecutter gh:utia-gc/scrnaseq --directory cookiecutter --checkout main\n</code></pre> <p>Simply follow the prompts and your project directory will be created.</p> </li> </ol>"},{"location":"input_output/project_setup/#nextflow-setup-pipeline","title":"Nextflow setup pipeline","text":"<ol> <li> <p>Install or update the latest pipeline revision:</p> Terminal<pre><code>nextflow pull utia-gc/scrnaseq\n</code></pre> </li> <li> <p>Run the project setup Nextflow script:</p> Terminal<pre><code>nextflow run utia-gc/scrnaseq \\\n    -main-script setup.nf \\\n    -revision main \\\n    -params-file src/nextflow/setup_params.yaml\n</code></pre> </li> </ol>"},{"location":"input_output/required_params/","title":"Required params","text":""},{"location":"input_output/required_params/#problem","title":"Problem","text":"<p>What params do I need to run the pipeline?</p> <p>Bioinformatics pipelines require input of user data and settings. We have designed <code>utia-gc/scrnaseq</code> and pipelines built on <code>utia-gc/ngs</code> to require limited params that are simple to specify.</p>"},{"location":"input_output/required_params/#solution","title":"Solution","text":"<p>We identified a few types of information that users must supply to be able to run the pipeline:</p> <ul> <li>Input sample files</li> <li><code>samplesheet</code> -- Sample files and metadata formatted as a structured CSV file</li> <li>Reference assembly information</li> <li><code>genome</code> -- A URL or path to a reference genome fasta file. May be gzip compressed.</li> <li><code>annotations</code> -- A URL path to a reference annotations GTF file. May be gzip compressed.</li> </ul>"},{"location":"input_output/required_params/#usage","title":"Usage","text":""},{"location":"input_output/samplesheet_format/","title":"Samplesheet format","text":""},{"location":"pipeline_config/","title":"Pipeline Configuration","text":""},{"location":"pipeline_config/arguments_to_processes/","title":"Arguments to processes","text":""},{"location":"pipeline_config/arguments_to_processes/#problem","title":"Problem","text":"<p>How do I run a step of the pipeline with the command line arguments that I want to use?</p> <p>Bioinformatics pipelines should have reasonable default arguments but ultimately be fully configurable by the user. We have designed <code>utia-gc/scrnaseq</code> and pipelines built on <code>utia-gc/ngs</code> to make handling command line arguments user-friendly, predictable, and robust.</p>"},{"location":"pipeline_config/arguments_to_processes/#solution","title":"Solution","text":"<p>We identified three levels at which configurable process-level arguments should be specified:</p> <ol> <li>Default arguments -- Arguments built into the pipeline that we find reasonable or deem to be a good starting point.</li> <li>Dynamic arguments -- Arguments determined by the pipeline at runtime. For example, a user analyzing an RNA-seq experiment where libraries are a mix of single-end and paired-end reads may want their pipeline run's read counting step to run in single- or paired-end mode depending on the type of library provided.</li> <li>User arguments -- Arguments specified by the user.</li> </ol> <p>A key consideration built in to <code>utia-gc/scrnaseq</code> is that arguments at each successive level should add to the arguments of the level above it, and if the same argument is supplied at multiple levels, the lower level should take precedence. Concretely, the order of precedence is as follows: user &gt; dynamic &gt; default.</p> <p>To illustrate this idea, consider a process for counting reads within features using a made up tool. The following three argument levels are set:</p> <ol> <li>Default arguments -- <code>--ends single --verbose 0</code></li> <li>Dynamic arguments -- <code>--ends paired --stranded 1</code></li> <li>User arguments -- <code>--verbose 2 --feature exon --stranded 0</code></li> </ol> <p><code>utia-gc/ngs</code> solves this collection of arguments so that the arguments that are finally used in the command are: <code>--ends paired --verbose 2 --stranded 0 --feature exon</code>. Here are the arguments that we had after each step of merging:</p> <ol> <li>Default -- <code>--ends single --verbose 0</code></li> <li>Dynamic &gt; default -- <code>--ends paired --verbose 0 --stranded 1</code></li> <li>User &gt; dynamic &gt; default -- <code>--ends paired --verbose 2 --stranded 0 --feature exon</code></li> </ol>"},{"location":"pipeline_config/arguments_to_processes/#usage","title":"Usage","text":"<p>The three levels of arguments are specified as Groovy map objects in the special <code>ext</code> process directive associated with a specific process name using Nextflow's <code>withName</code> process selector. This allows each process to have its own set of arguments that can easily be specified using a Nextflow configuration file.</p> <p>You can see an example of how the default and dynamic arguments are specified in the pipeline's builtin arguments configuration file.</p>"},{"location":"pipeline_config/arguments_to_processes/#how-to-specify-arguments","title":"How to specify arguments","text":"<p>Let's use an example to demonstrate how to specify arguments -- the same example as the read counting tool from above. In this example, the pipeline has the following builtin arguments in a configuration file:</p> pipeline-directory/conf/args.config<pre><code>process {\n    withName: 'foo' {\n        ext.argsDefault = [\n            '--ends': 'single',\n            '--verbose': '0',\n        ]\n        ext.argsDynamic = [\n            '--ends': \"${metadata.readType}\",\n            '--stranded': metadata.stranded ? '1' : '0',\n        ]\n    }\n}\n</code></pre> <p>The user can then specify their own arguments by adding arguments in their user configuration file:</p> nextflow.config<pre><code>process {\n    withName: 'foo' {\n        ext.argsUser = [\n            '--verbose': '2',\n            '--stranded': '0',\n            '--feature': 'exon',\n        ]\n    }\n}\n</code></pre> <p>As stated above, <code>utia-gc/scrnaseq</code> solves this collection of arguments (and transforms the <code>Map</code> to a <code>String</code> for its use in the command) so that the arguments that are finally used in the command are: <code>'--ends paired --verbose 2 --stranded 0 --feature exon'</code>.</p> <p>Note (as explained further below) that because of the dynamic arguments, this is true for the case in which <code>metadata.readType == 'paired'</code>. If <code>metadata.readType == 'single'</code>, then the arguments would be solved as: <code>'--ends single --verbose 2 --stranded 0 --feature exon'</code>. This is the power of the dynamic arguments.</p> <p>There are many things to point out here in this small example:</p> <ul> <li>The arguments are specified for the process named <code>foo</code> by using the <code>withName</code> process selector in the process scope in a Nextflow configuration file.</li> <li>Default arguments are specified in a map of <code>String:String</code> using the special variable name <code>ext.argsDefault</code>.</li> <li>Similarly, dynamic arguments are specifed in a map of <code>String:String</code> using the special variable name <code>ext.argsDynamic</code>.</li> <li>Similarly, user arguments are specifed in a map of <code>String:String</code> using the special variable name <code>ext.argsUser</code>.</li> <li>The dynamic arguments map can use Groovy code to access objects within the process scope and set arguments dynamically.</li> <li>For a less jargon heavy explanation, when the <code>foo</code> process runs for a sample, it uses the information associated with the run for that sample to determine arguments.<ul> <li>Example 1: Sample 'bar' has <code>metadata.readType == 'paired'</code>, then that part of its map will be <code>'--ends': 'paired'</code>.</li> <li>Example 2: Sample 'buzz' has <code>metadata.stranded == false</code>, then that part of its map will be <code>'--stranded': '0'</code>.</li> <li>This is an example of the Groovy ternary operator, which is essentially just a compact if/else branch.</li> </ul> </li> <li>This use of code and access of objects within the process scope is not specifc to <code>ext.argsDynamic</code> -- it can be done with any variable names within <code>ext</code>. However, we recommend reserving <code>ext.argsDynamic</code> for the use of this dynamic evaluation as a matter of convention so that it is easy to see where arguments that may be different for different samples are set.</li> <li>Trailing commas -- a comma after the last element in a map -- are allowed in Groovy, and we encourage their use. They don't do anything special, but their use can help avoid errors as in the case that additional arguments are added later, the user does not have to remember to add a comma before adding more elements to the map.</li> <li>By convention we recommend using full argument names instead of single letters when available. This drastically improves the readability of a project.</li> </ul>"},{"location":"pipeline_config/arguments_to_processes/#ideal-usage-only-change-user-args","title":"Ideal usage -- Only change user args","text":"<p>Ideally, the default and dynamic arguments for a process are acceptable and need not be changed.</p> <p>If the user does not desire to specify any additional arguments, then they do not need to take any action. If no user arguments are specified for a process, then the process is simply run with the default and dynamic arguments -- the user arguments default to an empty map which will not overwrite any default or dynamic arguments.</p>"},{"location":"pipeline_config/exploratory_profile/","title":"Exploratory","text":""},{"location":"pipeline_config/exploratory_profile/#the-exploratory-profile","title":"The <code>exploratory</code> profile","text":"<p>Analysis of single cell RNA-seq data frequently requires an exploratory stage that involves iterating through various parameter options and scrutinizing their effects on important QC metrics before settling on a final set of parameters. To help facilitate this crucial process, we have included an <code>exploratory</code> profile option which implements the following features:</p> <ul> <li>Data and reports are published within time-stamped subdirectories of <code>exploratory</code> structured as follows: <code>&lt;current working directory&gt;/exploratory/&lt;timestamp&gt;_&lt;project title&gt;</code>. This allows the user to see a chronological log of their changes and gives the option to put a brief description of changes in the project title.</li> <li>Results are published as symbolic links as opposed to the default behavior of copying published results. This prevents the user's working directory from being bloated with duplicates of data.</li> <li>Sets the <code>-resume</code> flag in Nextflow through the profile so that it does not need to be supplied at the command line. This allows for faster iteration and exploration as results from intensive processes are used from their cached location instead of being reproduced. For more info on Nextflow's resume feature, checkout these articles on demistifying and troubleshooting Nextflow resume.</li> </ul> <p>Once the user finishes exploring and has decided on a final set of parameters, those parameters should be specified during an explicitly resumed run of the pipeline without the <code>exploratory</code> profile. By default this will rerun the pipeline and publish results by copying them into the user's specified data and report publishing directories (see output documentation). This serves the dual purpose of saving time by not repeating logged tasks while aiding in data persistence.</p>"},{"location":"pipeline_config/exploratory_profile/#example-usage","title":"Example usage","text":"<p>During exploratory analysis, iteratively make changes to parameters and run the pipeline with the <code>exploratory</code> profile:</p> Terminal<pre><code>nextflow run utia-gc/scrnaseq \\\n   -revision main \\\n   -profile exploratory\n</code></pre> <p>Once you have settled on an optimal set of parameters, rerun the pipeline without the <code>exploratory</code> profile:</p> Terminal<pre><code>nextflow run utia-gc/scrnaseq \\\n   -revision main \\\n   -resume\n</code></pre> <p>Useful tip --- if a specific previous run contained the user's optimal set of parameter, or more generally if for some reason it would be advantageous to resume from some run of the pipeline other than the most recent run, then the pipeline can be resumed from any previous cached run using the RUN NAME or SESSION ID of the desired run. Use <code>nextflow log</code> to view information about previous runs. For example, to resume from a run named 'boring_euler':</p> Terminal<pre><code>nextflow run utia-gc/scrnaseq \\\n   -revision main \\\n   -resume boring_euler\n</code></pre>"},{"location":"pipeline_config/params/","title":"Params","text":"<p>A Nextflow pipeline for base NGS analysis.</p>"},{"location":"pipeline_config/params/#inputoutput-options","title":"Input/output options","text":"Parameter Description Type Default Required Hidden <code>projectTitle</code> A short title for the project. Used in naming files/directories. <code>string</code> True <code>samplesheet</code> Path to input samplesheet in CSV format. <code>string</code> True <code>genome</code> A path or URL to the reference genome sequence file in fasta format. HelpFor reasons of reproducibility and portability we recommend using direct links to reference genome sequence files available through repositories such as RefSeq and Ensembl. <code>string</code> True <code>annotations</code> A path or URL to the reference genome annotations file in GTF format. HelpFor reasons of reproducibility and portability we recommend using direct links to reference annotation files available through repositories such as RefSeq and Ensembl. <code>string</code> True <code>publishDirData</code> Base directory in which output data files will be published. HelpWe think of this pipeline as producing as producing two main types of output: data and reports. Data files are raw or processed files that are used for generating results and insights. They typically are not immediately interpretable by humans and often are not even human readable.Examples of data files would include raw fastq files, mappings in BAM format, tables of read counts within genes. <code>string</code> ${launchDir}/data <code>publishDirReports</code> Base directory in which output reports files will be published. HelpWe think of this pipeline as producing as producing two main types of output: data and reports. Reports files are processed files that are used for immediately producing insights. They typically immediately interpretable by humans.Examples of reports files would include MultiQC reports and MultiQC data tables, summary statistics files produced by Samtools stats, etc. <code>string</code> ${launchDir}/reports <code>publishMode</code> Method for publishing files. HelpThis sets the default mode for publishing files. See the <code>mode</code> section of the <code>publishDir</code> Nextflow docs for details about the options. <code>string</code> copy"},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"quick_start/#help-at-the-command-line","title":"Help at the command line","text":"<p>To get usage help for <code>utia-gc/scrnaseq</code> at the command line, specify the <code>--help</code> parameter:</p> Terminal<pre><code>nextflow run utia-gc/scrnaseq \\\n    -revision main \\\n    --help\n</code></pre>"}]}